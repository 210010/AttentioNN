# AttentioNN
All about attention in neural networks described as colab notebooks

## Notebooks

<table class="tg">
  <tr>
    <th class="tg-yw4l"><b>Name</b></th>
    <th class="tg-yw4l"><b>Description</b></th>
    <th class="tg-yw4l"><b>Notebook</b></th>
  </tr>
  <tr>
    <td class="tg-yw4l">Attention maps</td>
    <td class="tg-yw4l">How does a CNN attent to image objects </td>
    <td class="tg-yw4l"><a href="https://colab.research.google.com/github/zaidalyafeai/AttentioNN/blob/master/Attention_Maps.ipynb">
    <img src="https://colab.research.google.com/assets/colab-badge.svg" height = '20px' >
    </a></td>
  </tr>
  <tr>
    <td class="tg-yw4l">Attention in nmt</td>
    <td class="tg-yw4l">Attention mechanism in neural machine translation</td></td>
    <td class="tg-yw4l"><a href="https://colab.research.google.com/github/zaidalyafeai/AttentioNN/blob/master/Attention_in_NMT.ipynb">
    <img src="https://colab.research.google.com/assets/colab-badge.svg" height = '20px' >
    </a></td>
  </tr>
</table>
